{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESSpider(scrapy.Spider):\n",
    "    # Name is important, need different ones for each spider\n",
    "    # of the same class\n",
    "    name = 'ESS'\n",
    "    \n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    # Defining the scraping process\n",
    "    def parse(self, response):\n",
    "        with open('./scraper_results/mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "# Instantiate the crawler\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with the spider\n",
    "process.crawl(ESSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That spider downloaded the entire html code for the www.everydaysexism.com page to the specified directory.  However, to get more useful, parsed data, we must give the spider more specific instructions.\n",
    "\n",
    "**Remember that we need to restart the notebook kernel for the next spider to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = 'ESS'\n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Iterate over every <article> element on the page\n",
    "        for article in response.xpath('//article'):\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "            \n",
    "            \n",
    "# Pass in crawler parameters\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in json\n",
    "    'FEED_URI': './scraper_results/firstpage.json',  # Name of the json file\n",
    "    'LOG_ENABLED': False           # Turning off logging\n",
    "})\n",
    "\n",
    "# Start the crawler & spider\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "firstpage = pd.read_json('./scraper_results/firstpage.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we successfully scraped the first page of the site.  In order to scrape more pages, we need to recursively call the scraper as it's running.  We'll need to find the link to the next page and also institute some scraping etiquette rules to conform to internet norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = 'ESS'\n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "\n",
    "        # Iterate over every <article> element on the page\n",
    "        for article in response.xpath('//article'):\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "            # Getting the next page URL\n",
    "            next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "\n",
    "            # Grabbing the next page number\n",
    "            pagenum = int(re.findall(r'\\d+', next_page)[0])\n",
    "\n",
    "            # Recursively call the spider until page 9\n",
    "            if next_page is not None and pagenum < 10:\n",
    "                next_page = response.urljoin(next_page)\n",
    "                # Request next page with same parsing as above\n",
    "                yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "            \n",
    "# Pass in crawler parameters\n",
    "# Additional parameters are for scraping etiquette\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': './scraper_results/data.json',\n",
    "    'LOG_ENABLED': False,\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Instantiate and start crawler\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./scraper_results/data.json', orient='records')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool.  We sucessfully used Wikipedia's API to scrape specific info and store it.\n",
    "\n",
    "Let's try a less canned version.  We'll try to pull Nate Silver's articles from [FiveThirtyEight](https://fivethirtyeight.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re \n",
    "\n",
    "class FiveThirtyEight(scrapy.Spider):\n",
    "    name = \"NS\"\n",
    "    \n",
    "    start_urls = ['https://fivethirtyeight.com/contributors/nate-silver/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for item in response.xpath(\"//div[@class='content-area']/div\"):\n",
    "            yield {\n",
    "                'date': item.xpath(\".//div[@class='post-info']/p/time/text()\").extract_first(),\n",
    "                'title': item.xpath(\".//div[@class='post-info']/div/div/h2/a/text()\").extract_first(),\n",
    "                'article_link': item.xpath(\".//div[@class='post-info']/div/div/h2/a/@href\").extract_first(),\n",
    "                'author': item.xpath(\".//div[@class='post-info']/div/div/p[@class='single-metadata card vcard']/a/text()\").extract_first()\n",
    "            }\n",
    "        \n",
    "        nextpage = response.xpath(\"//div[@class='links']/a/@href\").extract_first()\n",
    "        pagenum = int(re.findall(r'\\d+', nextpage)[0])\n",
    "        \n",
    "        # Recursively call next page\n",
    "        if nextpage is not None and pagenum < 4: \n",
    "            nextpage = response.urljoin(nextpage)\n",
    "            yield scrapy.Request(nextpage, callback=self.parse)\n",
    "            \n",
    "            \n",
    "# Passing crawler parameters\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': './scraper_results/NS538.json',\n",
    "    'LOG_ENABLED': False,\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'Matt Francsis (mkfrancsis@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Instantiate and start scraper\n",
    "process.crawl(FiveThirtyEight)\n",
    "process.start()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://fivethirtyeight.com/features/silver-bu...</td>\n",
       "      <td>Nate Silver</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>\\n\\t\\t\\t\\tSilver Bulletpoints: Who’s In Danger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://fivethirtyeight.com/features/are-the-r...</td>\n",
       "      <td>Nate Silver</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>\\n\\t\\t\\t\\tAre The Raptors Really Favorites Aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://fivethirtyeight.com/features/is-there-...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>\\n\\t\\t\\t\\tIs There An Anti-Biden Lane In The D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link       author       date  \\\n",
       "0                                               None         None        NaT   \n",
       "1                                               None         None        NaT   \n",
       "2  https://fivethirtyeight.com/features/silver-bu...  Nate Silver 2019-05-30   \n",
       "3  https://fivethirtyeight.com/features/are-the-r...  Nate Silver 2019-05-29   \n",
       "4  https://fivethirtyeight.com/features/is-there-...         None 2019-05-29   \n",
       "\n",
       "                                               title  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  \\n\\t\\t\\t\\tSilver Bulletpoints: Who’s In Danger...  \n",
       "3  \\n\\t\\t\\t\\tAre The Raptors Really Favorites Aga...  \n",
       "4  \\n\\t\\t\\t\\tIs There An Anti-Biden Lane In The D...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./scraper_results/NS538.json', orient='records')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so the scraper works.  The `None` rows are *article-like* areas of the web page that are not actually articles while the articles with `None` as the author are podcasts or group chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
