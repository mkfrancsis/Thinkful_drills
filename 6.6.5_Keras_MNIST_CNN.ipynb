{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MNIST - CNN\n",
    "\n",
    "We'll run through the MNIST data set using a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to preprocess the data:\n",
    "- Split\n",
    "- Reshape\n",
    "- Type conversion\n",
    "- Scale\n",
    "- Convert class vectors to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dimensions.\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10 \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshaping\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "# Re-typing\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Scaling.\n",
    "x_train /= 255 \n",
    "x_test /= 255 \n",
    "\n",
    "# Target class binarization\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: ', x_train.shape)\n",
    "print(x_train.shape[0], 'Training samples')\n",
    "print(x_test.shape[0], 'Testing samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the model.  Initially we'll use the following architecture.  \n",
    "\n",
    "- Conv2D, depth=32, kernel=(3,3), relu\n",
    "- Conv2D, depth=64, kernel=(3,3), relu\n",
    "- MaxPooling2D, pooling=(2,2)\n",
    "- Dropout\n",
    "- Flatten\n",
    "- Dense, depth=128, relu\n",
    "- Dropout\n",
    "- Dense, depth=10, softmax\n",
    "\n",
    "Additionally, we'll use a categorical cross-entropy loss function and Ada delta optimizer, targeting accuracy.  Batch size will be 128 over ten training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mkfrancsis/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.2663 - acc: 0.9177 - val_loss: 0.0591 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.0894 - acc: 0.9737 - val_loss: 0.0391 - val_acc: 0.9867\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.0653 - acc: 0.9802 - val_loss: 0.0389 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0532 - acc: 0.9843 - val_loss: 0.0296 - val_acc: 0.9888\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.0463 - acc: 0.9860 - val_loss: 0.0294 - val_acc: 0.9898\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.0392 - acc: 0.9879 - val_loss: 0.0299 - val_acc: 0.9904\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0362 - acc: 0.9889 - val_loss: 0.0298 - val_acc: 0.9902\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0327 - acc: 0.9898 - val_loss: 0.0284 - val_acc: 0.9904\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0308 - acc: 0.9908 - val_loss: 0.0266 - val_acc: 0.9911\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0291 - acc: 0.9910 - val_loss: 0.0279 - val_acc: 0.9912\n",
      "10000/10000 [==============================] - 2s 248us/step\n",
      "Test loss:  0.027948839039341693\n",
      "Test accuracy:  0.9912\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss=categorical_crossentropy, \n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model training\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10, \n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
